{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Models - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice many concepts learned in this section, from adding interactions and polynomials to your model to AIC and BIC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You will be able to:\n",
    "- Build a linear regression model with polynomial features/interactions\n",
    "- Perform regularization\n",
    "- Use AIC and BIC to select the best value for the regularization parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at a Baseline Boston Housing Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Boston housing data set, use all the predictors in their scaled version (using `preprocessing.scale`. Look at a baseline model using *scaled variables* as predictors. Use 5-fold cross-validation this time and use the $R^2$ score to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_boston().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X = load_boston().data\n",
    "y = load_boston().target\n",
    "features = load_boston().feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7176324491383014 0.7176324491383005\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "cross_val_scaled = cross_val_score(linreg, X_scaled, y, scoring='r2', n_jobs=-1, cv=crossvalidation)\n",
    "cross_val_reg = cross_val_score(linreg, X, y, scoring='r2', n_jobs=-1, cv=crossvalidation)\n",
    "\n",
    "baseline_scaled = np.mean(cross_val_scaled)\n",
    "baseline_reg = np.mean(cross_val_reg)\n",
    "\n",
    "print(baseline_reg, baseline_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include interactions\n",
    "\n",
    "Look at all the possible combinations of variables for interactions by adding interactions one by one to the baseline model. Next, evaluate that model using 5-fold classification and store the $R^2$ to compare it with the baseline model.\n",
    "\n",
    "You've created code for this before in the interactions lab, yet this time, you have scaled the variables so the outcomes may look different. \n",
    "\n",
    "Print the 7 most important interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.DataFrame(X, columns=features)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "y = pd.DataFrame(y, columns=['target'])\n",
    "df_all = pd.concat([df_scaled, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CRIM', 'ZN'), ('CRIM', 'INDUS'), ('CRIM', 'CHAS')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_combinations = list(combinations(features, 2))\n",
    "feature_combinations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 interactions: [('RM', 'LSTAT', 0.783), ('RM', 'TAX', 0.775), ('RM', 'RAD', 0.77), ('RM', 'PTRATIO', 0.764), ('INDUS', 'RM', 0.757), ('NOX', 'RM', 0.746), ('RM', 'AGE', 0.742)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\"\"\"\n",
    "Look at all the possible combinations of variables for interactions by adding \n",
    "interactions one by one to the baseline model. Next, evaluate that model using 5-fold \n",
    "classification and store the  R2R2  to compare it with the baseline model.\n",
    "\"\"\"\n",
    "interactions = []\n",
    "data = df_scaled.copy()\n",
    "for comb in feature_combinations:\n",
    "    data[\"interaction\"] = data[comb[0]] * data[comb[1]]\n",
    "    score = np.mean(cross_val_score(linreg, data, y, scoring=\"r2\", cv=crossvalidation))\n",
    "    if score > baseline_scaled: \n",
    "        interactions.append((comb[0], comb[1], round(score,3)))\n",
    "            \n",
    "print(\"Top 7 interactions: %s\" %sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 interactions: [('RM', 'LSTAT', 0.783), ('RM', 'TAX', 0.775), ('RM', 'RAD', 0.77), ('RM', 'PTRATIO', 0.764), ('INDUS', 'RM', 0.757), ('NOX', 'RM', 0.746), ('RM', 'AGE', 0.742)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 7 interactions: %s\" %sorted_interactions[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 interactions: [('RM', 'LSTAT', 0.783), ('RM', 'TAX', 0.775), ('RM', 'RAD', 0.77), ('RM', 'PTRATIO', 0.764), ('INDUS', 'RM', 0.757), ('NOX', 'RM', 0.746), ('RM', 'AGE', 0.742)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 7 interactions: {}\".format(sorted_interactions[:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to include the 7 most important interactions in your data set by adding 7 columns. Name the columns \"var1_var2\" with var1 and var2 the two variables in the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "sorted_interactions = sorted(interactions, key=lambda inter: inter[2], reverse=True)\n",
    "df_scaled_new = df_scaled.copy()\n",
    "count = 0\n",
    "limit = 7\n",
    "for interaction in sorted_interactions:\n",
    "    col_name = '_'.join([interaction[0], interaction[1]])\n",
    "    df_scaled_new[col_name] = df_scaled_new[interaction[0]] * df_x_scaled_new[interaction[1]]\n",
    "    count += 1\n",
    "    if count == 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>RM_LSTAT</th>\n",
       "      <th>RM_TAX</th>\n",
       "      <th>RM_RAD</th>\n",
       "      <th>RM_PTRATIO</th>\n",
       "      <th>INDUS_RM</th>\n",
       "      <th>NOX_RM</th>\n",
       "      <th>RM_AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.417713</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "      <td>-0.444930</td>\n",
       "      <td>-0.275757</td>\n",
       "      <td>-0.406574</td>\n",
       "      <td>-0.603547</td>\n",
       "      <td>-0.532772</td>\n",
       "      <td>-0.059659</td>\n",
       "      <td>-0.049646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.415269</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "      <td>-0.095668</td>\n",
       "      <td>-0.191813</td>\n",
       "      <td>-0.168607</td>\n",
       "      <td>-0.058883</td>\n",
       "      <td>-0.115279</td>\n",
       "      <td>-0.143814</td>\n",
       "      <td>0.071331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.415272</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "      <td>-1.550451</td>\n",
       "      <td>-1.266461</td>\n",
       "      <td>-1.113245</td>\n",
       "      <td>-0.388783</td>\n",
       "      <td>-0.761138</td>\n",
       "      <td>-0.949544</td>\n",
       "      <td>-0.340960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.414680</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "      <td>-1.383713</td>\n",
       "      <td>-1.124148</td>\n",
       "      <td>-0.765197</td>\n",
       "      <td>0.114875</td>\n",
       "      <td>-1.328183</td>\n",
       "      <td>-0.848901</td>\n",
       "      <td>-0.823092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.410409</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "      <td>-1.261136</td>\n",
       "      <td>-1.358947</td>\n",
       "      <td>-0.925023</td>\n",
       "      <td>0.138869</td>\n",
       "      <td>-1.605599</td>\n",
       "      <td>-1.026210</td>\n",
       "      <td>-0.628023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0 -0.417713  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1 -0.415269 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2 -0.415272 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3 -0.414680 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4 -0.410409 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT  RM_LSTAT  \\\n",
       "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562 -0.444930   \n",
       "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439 -0.095668   \n",
       "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727 -1.550451   \n",
       "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517 -1.383713   \n",
       "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501 -1.261136   \n",
       "\n",
       "     RM_TAX    RM_RAD  RM_PTRATIO  INDUS_RM    NOX_RM    RM_AGE  \n",
       "0 -0.275757 -0.406574   -0.603547 -0.532772 -0.059659 -0.049646  \n",
       "1 -0.191813 -0.168607   -0.058883 -0.115279 -0.143814  0.071331  \n",
       "2 -1.266461 -1.113245   -0.388783 -0.761138 -0.949544 -0.340960  \n",
       "3 -1.124148 -0.765197    0.114875 -1.328183 -0.848901 -0.823092  \n",
       "4 -1.358947 -0.925023    0.138869 -1.605599 -1.026210 -0.628023  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try polynomials of 2, 3 and 4 for each variable, in a similar way you did for interactions (by looking at your baseline model and seeing how $R^2$ increases). Do understand that when going for a polynomial of 4, the particular column is raised to the power of 2 and 3 as well in other terms. We only want to include \"pure\" polynomials, so make sure no interactions are included. We want the result to return a list that contain tuples of the form:\n",
    "\n",
    "`(var_name, degree, R2)`, so eg. `('DIS', 3, 0.732)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 polynomials: [('RM', 4, 0.8), ('RM', 2, 0.782), ('LSTAT', 4, 0.782), ('RM', 3, 0.781), ('LSTAT', 3, 0.774), ('LSTAT', 2, 0.772), ('DIS', 3, 0.737), ('DIS', 2, 0.732), ('DIS', 4, 0.731), ('TAX', 4, 0.724)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "polynomials = []\n",
    "for col in df_reg.columns:\n",
    "    for degree in [2,3,4]:\n",
    "        data = df_scaled.copy()\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X = poly.fit_transform(df_reg[[col]])\n",
    "        data = pd.concat([data.drop(col, axis=1),pd.DataFrame(X)], axis = 1)\n",
    "        score = np.mean(cross_val_score(linreg, data, y, scoring=\"r2\", cv=crossvalidation))\n",
    "        if score > baseline_scaled: \n",
    "            polynomials.append((col, degree, round(score,3)))\n",
    "print(\"Top 10 polynomials: %s\" %sorted(polynomials, key=lambda poly: poly[2], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each variable, print out the maximum R2 possible when including Polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "ZN         0.723\n",
       "INDUS      0.723\n",
       "CHAS       0.718\n",
       "NOX        0.721\n",
       "RM         0.800\n",
       "AGE        0.722\n",
       "DIS        0.737\n",
       "RAD        0.719\n",
       "TAX        0.724\n",
       "PTRATIO    0.721\n",
       "B          0.720\n",
       "LSTAT      0.782\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "polynom = pd.DataFrame(polynomials)\n",
    "polynom.groupby([0], sort=False)[2].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which two variables seem to benefit most from adding Polynomial terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Polynomials for the two features that seem to benefit the most, as in have the best R squared compared to the baseline model. For each of the two feature, raise to the Polynomial that generates the best result. Make sure to start from the data set `df_inter` so the final data set has both interactions and polynomials in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "for col in [\"RM\", \"LSTAT\"]:\n",
    "    poly = PolynomialFeatures(4, include_bias=False)\n",
    "    X = poly.fit_transform(df_reg[[col]])\n",
    "    colnames= [col, col+\"_\"+\"2\", col+\"_\"+\"3\", col+\"_\"+\"4\"]\n",
    "    df_scaled_new = pd.concat([df_scaled_new.drop(col, axis=1),pd.DataFrame(X, columns=colnames)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out your final data set and make sure that your interaction terms as well as your polynomial terms are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>NOX_RM</th>\n",
       "      <th>RM_AGE</th>\n",
       "      <th>RM</th>\n",
       "      <th>RM_2</th>\n",
       "      <th>RM_3</th>\n",
       "      <th>RM_4</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>LSTAT_2</th>\n",
       "      <th>LSTAT_3</th>\n",
       "      <th>LSTAT_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.417713</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059659</td>\n",
       "      <td>-0.049646</td>\n",
       "      <td>6.575</td>\n",
       "      <td>43.230625</td>\n",
       "      <td>284.241359</td>\n",
       "      <td>1868.886938</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.8004</td>\n",
       "      <td>123.505992</td>\n",
       "      <td>615.059840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.415269</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143814</td>\n",
       "      <td>0.071331</td>\n",
       "      <td>6.421</td>\n",
       "      <td>41.229241</td>\n",
       "      <td>264.732956</td>\n",
       "      <td>1699.850313</td>\n",
       "      <td>9.14</td>\n",
       "      <td>83.5396</td>\n",
       "      <td>763.551944</td>\n",
       "      <td>6978.864768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.415272</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.949544</td>\n",
       "      <td>-0.340960</td>\n",
       "      <td>7.185</td>\n",
       "      <td>51.624225</td>\n",
       "      <td>370.920057</td>\n",
       "      <td>2665.060607</td>\n",
       "      <td>4.03</td>\n",
       "      <td>16.2409</td>\n",
       "      <td>65.450827</td>\n",
       "      <td>263.766833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.414680</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848901</td>\n",
       "      <td>-0.823092</td>\n",
       "      <td>6.998</td>\n",
       "      <td>48.972004</td>\n",
       "      <td>342.706084</td>\n",
       "      <td>2398.257176</td>\n",
       "      <td>2.94</td>\n",
       "      <td>8.6436</td>\n",
       "      <td>25.412184</td>\n",
       "      <td>74.711821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.410409</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.026210</td>\n",
       "      <td>-0.628023</td>\n",
       "      <td>7.147</td>\n",
       "      <td>51.079609</td>\n",
       "      <td>365.065966</td>\n",
       "      <td>2609.126456</td>\n",
       "      <td>5.33</td>\n",
       "      <td>28.4089</td>\n",
       "      <td>151.419437</td>\n",
       "      <td>807.065599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX       AGE       DIS  \\\n",
       "0 -0.417713  0.284830 -1.287909 -0.272599 -0.144217 -0.120013  0.140214   \n",
       "1 -0.415269 -0.487722 -0.593381 -0.272599 -0.740262  0.367166  0.557160   \n",
       "2 -0.415272 -0.487722 -0.593381 -0.272599 -0.740262 -0.265812  0.557160   \n",
       "3 -0.414680 -0.487722 -1.306878 -0.272599 -0.835284 -0.809889  1.077737   \n",
       "4 -0.410409 -0.487722 -1.306878 -0.272599 -0.835284 -0.511180  1.077737   \n",
       "\n",
       "        RAD       TAX   PTRATIO     ...         NOX_RM    RM_AGE     RM  \\\n",
       "0 -0.982843 -0.666608 -1.459000     ...      -0.059659 -0.049646  6.575   \n",
       "1 -0.867883 -0.987329 -0.303094     ...      -0.143814  0.071331  6.421   \n",
       "2 -0.867883 -0.987329 -0.303094     ...      -0.949544 -0.340960  7.185   \n",
       "3 -0.752922 -1.106115  0.113032     ...      -0.848901 -0.823092  6.998   \n",
       "4 -0.752922 -1.106115  0.113032     ...      -1.026210 -0.628023  7.147   \n",
       "\n",
       "        RM_2        RM_3         RM_4  LSTAT  LSTAT_2     LSTAT_3      LSTAT_4  \n",
       "0  43.230625  284.241359  1868.886938   4.98  24.8004  123.505992   615.059840  \n",
       "1  41.229241  264.732956  1699.850313   9.14  83.5396  763.551944  6978.864768  \n",
       "2  51.624225  370.920057  2665.060607   4.03  16.2409   65.450827   263.766833  \n",
       "3  48.972004  342.706084  2398.257176   2.94   8.6436   25.412184    74.711821  \n",
       "4  51.079609  365.065966  2609.126456   5.33  28.4089  151.419437   807.065599  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "df_scaled_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the R-squared of the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8061116489236934"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "full_model = np.mean(cross_val_score(linreg, df_scaled_new, y, scoring=\"r2\", cv=crossvalidation))\n",
    "full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best Lasso regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've learned that, when using Lasso regularization, your coefficients shrink to 0 when using a higher regularization parameter. Now the question is which value we should choose for the regularization parameter. \n",
    "\n",
    "This is where the AIC and BIC come in handy! We'll use both criteria in what follows and perform cross-validation to select an optimal value of the regularization parameter alpha of the Lasso estimator.\n",
    "\n",
    "Read the page here: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html and create a similar plot as the first one listed on the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Information-criterion for model selection')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(df_scaled_new, y)\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(df_scaled_new, y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "\n",
    "def plot_ic_criterion(model, name, color):\n",
    "    alpha_ = model.alpha_\n",
    "    alphas_ = model.alphas_\n",
    "    criterion_ = model.criterion_\n",
    "    plt.plot(-np.log10(alphas_), criterion_, '--', color=color, linewidth=2, label= name)\n",
    "    plt.axvline(-np.log10(alpha_), color=color, linewidth=2,\n",
    "                label='alpha for %s ' % name)\n",
    "    plt.xlabel('-log(alpha)')\n",
    "    plt.ylabel('criterion')\n",
    "\n",
    "plt.figure()\n",
    "plot_ic_criterion(model_aic, 'AIC', 'green')\n",
    "plot_ic_criterion(model_bic, 'BIC', 'blue')\n",
    "plt.legend()\n",
    "plt.title('Information-criterion for model selection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the final result\n",
    "\n",
    "Finally, use the best value for regularization parameter according to AIC and BIC and compare the R squared parameters and MSE using train-test-split. Compare with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.7632796668701112\n",
      "Testing r^2: 0.6659539974224437\n",
      "Training MSE: 19.324759756564426\n",
      "Testing MSE: 30.96213268290194\n"
     ]
    }
   ],
   "source": [
    "# Code for baseline model\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "linreg_all = LinearRegression()\n",
    "linreg_all.fit(X_train, y_train)\n",
    "print('Training r^2:', linreg_all.score(X_train, y_train))\n",
    "print('Testing r^2:', linreg_all.score(X_test, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg_all.predict(X_train)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, linreg_all.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.8302298227240487\n",
      "Testing r^2: 0.7609996233510434\n",
      "Training MSE: 14.205620500619458\n",
      "Testing MSE: 20.63798229284935\n"
     ]
    }
   ],
   "source": [
    "# code for lasso with alpha from AIC\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled_new, y)\n",
    "\n",
    "lasso = Lasso(alpha= model_aic.alpha_) \n",
    "lasso.fit(X_train, y_train)\n",
    "print('Training r^2:', lasso.score(X_train, y_train))\n",
    "print('Testing r^2:', lasso.score(X_test, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.8292938475138585\n",
      "Testing r^2: 0.7659128201888941\n",
      "Training MSE: 14.283938782706995\n",
      "Testing MSE: 20.213721583462355\n"
     ]
    }
   ],
   "source": [
    "# code for lasso with alpha from BIC\n",
    "lasso = Lasso(alpha= model_bic.alpha_) \n",
    "lasso.fit(X_train, y_train)\n",
    "print('Training r^2:', lasso.score(X_train, y_train))\n",
    "print('Testing r^2:', lasso.score(X_test, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up - Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Lasso Path\n",
    "\n",
    "From this section, you know that when using lasso, more parameters shrink to zero as your regularization parameter goes up. In Scikit-Learn there is a function lasso_path which visualizes the shrinkage of the coefficients while alpha changes. Try this out yourself!\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC and BIC for subset selection\n",
    "This notebook shows how you can use AIC and BIC purely for feature selection. Try this code out on our Boston Housing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xavierbourretsicotte.github.io/subset_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to create better linear models and how to use AIC and BIC for both feature selection and to optimize your regularization parameter when performing Ridge and Lasso. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
